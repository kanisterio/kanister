apiVersion: cr.kanister.io/v1alpha1
kind: Blueprint
metadata:
  name: kafka-blueprint
  namespace: kasten-io
actions:
  backup:
    type: Deployment
    outputArtifacts:
      s3Dump:
        keyValue:
          s3path: '{{ .Phases.setupPhase.Output.s3path }}'
    phases:
    - func: KubeTask
      name: setupPhase
      args:
        namespace: "{{ .Object.metadata.namespace }}"
        podOverride:
          containers:
          - name: container
            imagePullPolicy: Always
          restartPolicy: Never
        image: akankshakumari393/confluent-kafka-s3:latest
        command:
          - bash
          - -o
          - errexit
          - -o
          - pipefail
          - -c
          - |
            mkdir /tmp/config
            {{- if .Profile.Credential.KeyPair }}
            export AWS_SECRET_KEY="{{ .Profile.Credential.KeyPair.Secret }}"
            export AWS_ACCESS_KEY="{{ .Profile.Credential.KeyPair.ID }}"
            {{- else }}
            export AWS_SECRET_KEY="{{ .Profile.Credential.Secret.Data.aws_secret_access_key | toString }}"
            export AWS_ACCESS_KEY="{{ .Profile.Credential.Secret.Data.aws_access_key_id | toString }}"
            {{- end }}
            region="{{ .Profile.Location.Region }}"
            bucket="{{ .Profile.Location.Bucket }}"
            s3config="{{ index .Object.data "s3Sink.properties" | toString }}"
            echo -e "${s3config}\ns3.region=${region}\ns3.bucket.name=${bucket}\n" > /tmp/config/s3config.properties

            s3folder=`cat /tmp/config/s3config.properties | grep "topics.dir=" | awk -F "=" '{print $2}'`
            s3_topic_path="${s3folder}_{{ .Time | date "2006-01-02T15-04-05" }}"

            sed -i "/^topics.dir/d" /tmp/config/s3config.properties


            echo -e "\ntopics.dir=${s3_topic_path}\n" >> /tmp/config/s3config.properties

            export s3_path="s3://{{ .Profile.Location.Bucket }}/${s3_topic_path}"

            kafkaConfig="{{ index .Object.data "kafkaConfiguration.properties" | toString }}"
            echo "$kafkaConfig" > /tmp/config/kafkaConfig.properties

            export timeinSeconds="{{ index .Object.data "timeinSeconds" | toString }}"
            export connectorName="`cat /tmp/config/s3config.properties | grep "^name=" | awk -F "=" '{print $2}'`"

            export bootstrapServer=`cat /tmp/config/kafkaConfig.properties | grep "bootstrap.servers=" | awk -F "=" '{print $2}'`

            echo "============ENV variable set====================="
            /bin/connect-standalone /tmp/config/kafkaConfig.properties /tmp/config/s3config.properties &
            export pid=$!
            sh monitorconnect.sh
            exit 0
  restore:
    type: Deployment
    inputArtifactNames:
    - s3Dump
    phases:
    - func: KubeTask
      name: restorePreHookPhase
      args:
        namespace: "{{ .Object.metadata.namespace }}"
        podOverride:
          containers:
          - name: container
            imagePullPolicy: Always
        image: akankshakumari393/confluent-kafka-s3source:latest
        command:
        - bash
        - -o
        - errexit
        - -o
        - pipefail
        - -c
        - |
          mkdir /tmp/config
          {{- if .Profile.Credential.KeyPair }}
          export AWS_SECRET_KEY="{{ .Profile.Credential.KeyPair.Secret }}"
          export AWS_ACCESS_KEY="{{ .Profile.Credential.KeyPair.ID }}"
          {{- else }}
          export AWS_SECRET_KEY="{{ .Profile.Credential.Secret.Data.aws_secret_access_key | toString }}"
          export AWS_ACCESS_KEY="{{ .Profile.Credential.Secret.Data.aws_access_key_id | toString }}"
          {{- end }}
          export region="{{ .Profile.Location.Region }}"
          export bucket="{{ .Profile.Location.Bucket }}"
          env > /tmp/env.txt
          s3config="{{ index .Object.data "s3Source.properties" | toString }}"
          echo "${s3config}" > /tmp/config/s3config.properties

          s3_path="{{ .ArtifactsIn.s3Dump.KeyValue.s3path }}"
          export topicsDir="$(echo $s3_path | awk -F "/" '{print $4}')"
          echo -e "\ntopics.dir=${topicsDir}\n" >> /tmp/config/s3config.properties
          sleep 10
          python3 getTopicNames.py > /tmp/config/topics.txt
          while IFS= read -r topicPath
          do
              # getting topic name from configuration file
              export topic="$(echo "$topicPath" | cut -d/ -f 2)"
              echo "purging topic $topic"
              # getting bootstrapServer value from configuration file
              export bootstrapServer=`cat /tmp/config/s3config.properties | grep "confluent.topic.bootstrap.servers=" | cut -d'=' -f2`
              # getting retention period as set for the topic
              export retentionPeriod="$(/bin/kafka-configs --describe --bootstrap-server "$bootstrapServer" --entity-type topics --entity-name "$topic" --all | grep -m1 retention.ms= | sed 's/[^0-9]*//; s/ .*//')"
              # purging topic by setting retention to 1ms
              /bin/kafka-configs --bootstrap-server "$bootstrapServer" --entity-type topics --entity-name "$topic" --alter --add-config retention.ms=1
              echo "retention set to 1"
              # Verifying Purging is complete
              export startOffset="$(/bin/kafka-run-class kafka.tools.GetOffsetShell --broker-list "$bootstrapServer" --topic "$topic" --time -1)"
              export endoffset="$(/bin/kafka-run-class kafka.tools.GetOffsetShell --broker-list "$bootstrapServer" --topic "$topic" --time -2)"
              until [ "$startOffset" = "$endoffset" ]
              do
              echo "purging in process"
              startOffset="$(/bin/kafka-run-class kafka.tools.GetOffsetShell --broker-list "$bootstrapServer" --topic "$topic" --time -1)"
              endoffset="$(/bin/kafka-run-class kafka.tools.GetOffsetShell --broker-list "$bootstrapServer" --topic "$topic" --time -2)"
              sleep 1
              done
              echo "purging complete for topic $topic"
              echo "resetting the retention to previous value"
              # reset the retention period to previous value
              /bin/kafka-configs --bootstrap-server "$bootstrapServer" --entity-type topics --entity-name "$topic" bin/kafka-configs.sh --alter --add-config retention.ms="$retentionPeriod"
          done < "/tmp/config/topics.txt"
    - func: KubeTask
      name: restorePhase
      args:
        namespace: "{{ .Object.metadata.namespace }}"
        podOverride:
          containers:
          - name: container
            imagePullPolicy: Always
        image: akankshakumari393/confluent-kafka-s3source:latest
        command:
          - bash
          - -o
          - errexit
          - -o
          - pipefail
          - -c
          - |
            mkdir /tmp/config
            {{- if .Profile.Credential.KeyPair }}
            export AWS_SECRET_KEY="{{ .Profile.Credential.KeyPair.Secret }}"
            export AWS_ACCESS_KEY="{{ .Profile.Credential.KeyPair.ID }}"
            {{- else }}
            export AWS_SECRET_KEY="{{ .Profile.Credential.Secret.Data.aws_secret_access_key | toString }}"
            export AWS_ACCESS_KEY="{{ .Profile.Credential.Secret.Data.aws_access_key_id | toString }}"
            {{- end }}
            REGION="{{ .Profile.Location.Region }}"
            BUCKET="{{ .Profile.Location.Bucket }}"
            s3config="{{ index .Object.data "s3Source.properties" | toString }}"
            echo -e "${s3config}\ns3.region=${region}\ns3.bucket.name=${bucket}\n" > /tmp/config/s3config.properties

            kafkaConfig="{{ index .Object.data "kafkaConfiguration.properties" | toString }}"
            echo "$kafkaConfig" > /tmp/config/kafkaConfig.properties

            s3_path="{{ .ArtifactsIn.s3Dump.KeyValue.s3path }}"
            export topicsDir="$(echo $s3_path | awk -F "/" '{print $4}')"
            echo -e "\ntopics.dir=${topicsDir}\n" >> /tmp/config/s3config.properties

            # start kafka source connector
            sh /bin/connect-standalone /tmp/config/kafkaConfig.properties /tmp/config/s3config.properties
